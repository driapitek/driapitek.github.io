---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Моделирование

Знакомимся с Exploratory data analysis (EDA) --- разведочный анализ данных.

Цель моделирования --- получение простых суммарных (сводных) характеристик набора данных.

Здесь разбриаем предсказательные модели анализа данных. Есть ещё много разных, например data discovery, но их здесь не рассматриваем

Базовые принципы корректного статистического вывода:

* Каждое наблюдение можно использовать либо для разведочного анализа, либо для подтверждения гипотезы. Но не для того и другого одновременно

* В целях разведочного анализа допускается многократное использование наблюдения, но в целях подтверждения гипотез --- только однократное. Использование одного и того же наблюдения более одного раза равносильно переходу от подтверждения гипотезы к разведочному анализу.

Данные используемые для подтверждения гипотезы не должны зависеть от данных на основании которых эта гипотеза была выдвинута.

### Выдвижение гипотез и их подтверждение

Один из подходов к анализу, направленному на подтверждение гипотезы, заключается в разбиении данных на три части ещё до того, как приступить к анализу

* $60%$ --- training set. С этим набором можноделать всё что пожелается.

* $20%$ --- query set. Набор для сравнения моделей или вариантов визуализации вручную, но их не разрешается использовать в качестве части автоматизированного процесса.

* $20%$ --- test set. Эти данные можно использовать только один раз для тестирования окончательной модели.

## Базовое моделирование при помощи пакета `modelr`
### Введение

Цель моделирования --- определение суммарных характеристик набора данных. 

Моделирование включает два аспекта

1. Прежде всего необходимо определить семейство моделей. Они определяют типичную закономерность, например линейную или квадратичную зависимость. Семейство моделей описывается уравнением наподобие `y = a_1 * x + a_2` or `y = a_1 * x ^ a_2`. Где `x` и `y` --- известные переменные, а `a_1` и `a_2` варьируемые параметры.

2. Далее вы генерируете подходяющую модель, выбирая ту из семейства моделей, которая больше всего соответствует данным. В результате этого обобщенного уравнения семейства моделей конкретизируется принимая вид наподобие `y = 3 * x + 7`

Целью моделирования является не установление окончательной истины, а нахождение простого, но тем не менее полезного приближения


#### Необходимые ресурсы

```{r}
library(tidyverse)

library(modelr)
options(na.action = na.warn)
```

### Простая модель

Демонстрационный набор данный `sim1` содержит две непрерывные переменные `x` и `y`. Отложим их на графике, чтобы увидеть, как они связаны между собой

```{r}
ggplot(sim1, aes(x, y)) +
  geom_point()
```

между переменными наблюдается сильная взаимосвязь. Коэффициент корреляции

```{r}
cor(sim1$x, sim1$y)
```

Опишем эту взаимосвязь с помощью модели и выразим ее в явном виде.

Наша задача --- предоставить базовую форму модели. В данном случае мы используем линейное соотношение между переменными `y = a_0 + a_1 * x`.
Начнем с получения общего представления о том, как выглядят модели этого семейства. Путём случайной генерации некоторых из них и наложения их на данные.

В нашем случае можно использовать `geom_abline` --- она принимает наклон (slope), и y-пересечение (intercept).

```{r}
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

ggplot(sim1, aes(x, y)) + 
  geom_abline(
    aes(intercept = a1, slope = a2), 
    data = models, alpha = 1/4
  ) +
  geom_point()
```

Большинство из построенных моделей совсем неудачны. Нам нужен метод обеспечивающий количественную оценку расстояния между данными и моделью.

Это расстояние --- разность между значением предоставляемым моделью --- прогноз, и фактическим значением `y` согласно данным `data` --- результат.

![расстояния от результата до прогноза](img/model.png)

Для вычисления расстояния мы прежде всего превратим семейство моделей в функцию. 
В качестве аргументов, эта функция принимает параметры модели и данные, а в качестве выходного результата предоставляет значения, предсказанные моделью

```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

model1(c(7, 1.5), sim1)
```

Чтобы найти минимальное расстояние при помощи среднеквадратического отклонения.

```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}
measure_distance(c(7, 1.5), sim1)
```

Теперь используем пакет `purrr` для того чтобы вычислить это расстояние для всех ранее определённых моделей. Нам нужна вспомогательная функция, поскольку наша функция вычисляющая расстояние лжидает модель в качестве числового вектора с длиной 2

```{r}
sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}

models <- models %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
models
```

Выделим лучшие модели

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, color = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, color = -dist), 
    data = filter(models, rank(dist) <= 10)
  )
```

Мы можем рассматривать эти модели как наблюдения и визуализировать их с помощью точечной диаграмы с осями `a1` и `a2`, опять таки расцветив их в соответстсвии со значениями `-dist`. При этом мы увидим множество моделей одновременно.
Выделим 10 лучшиъ моделей кржочками

```{r}
ggplot(models, aes(a1, a2)) +
  geom_point(data = filter(models, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))
```

Можно так же построить сетку поиска! Где точки расположены не хаотично, а упорядоченно

```{r}
grid <- expand.grid(
  a1 = seq(-2, 10, length = 25),
  a2 = seq(0, 2, length = 25)
  ) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>% 
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist)) 
```

Если наложить эти 10 наилучших моделей на исходные данные, то они будут выглядеть довольно прилично

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(grid, rank(dist) <= 10)
  )
```

но конечно самый простой способ это воспользоваться  функцией `lm()` для построения линейных моделей

```{r}
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
```

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = coef(sim1_mod)[1], slope = coef(sim1_mod)[2])
```

#### Упражнение 23.2.1.1

<div class="question">
One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?

```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
```
</div>

Линейные модели неустойчивы к выбросам --- известный факт.

Попробуем это на одиночной модели

```{r}
sim1a_mod <- lm(y ~ x, sim1a)

ggplot(sim1a, aes(x, y)) + 
  geom_point(size = 2, color = "grey30") + 
  geom_abline(intercept = coef(sim1a_mod)[1], slope = coef(sim1a_mod)[2], color = "red")
```

Сделаем функцию, которая бы генерировала несколько распределений сразу

```{r}
simt <- function(i) {
  tibble(
    x = rep(1:10, each = 3),
    y = x * 1.5 + 6 + rt(length(x), df = 2),
    id = i
  )
}

sims <- map_df(1:15, simt)

ggplot(sims, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  facet_wrap(~id, ncol = 5)
```

Что если проделать всё тоже самое с нормальным распределением

```{r}
sim_norm <- function(i) {
  tibble(
    x = rep(1:10, each = 3),
    y = x * 1.5 + 6 + rnorm(length(x)),
    .id = i
  )
}

simdf_norm <- map_df(1:12, sim_norm)

ggplot(simdf_norm, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "red") +
  facet_wrap(~.id, ncol = 4)
```

Здесь не большие выбросы, а склоны больше похожи.

Причина в том, что t-распределение Стьюдента, из которого мы выбираем с помощью `rt()`, имеет более тяжелые хвосты, чем нормальное распределение `rnorm()`. Это означает, что t-распределение Стьюдента присваивает большую вероятность значениям дальше от центра распределения.

```{r}
tibble(
  x = seq(-5, 5, length.out = 100),
  normal = dnorm(x),
  student_t = dt(x, df = 2)
) %>%
  gather(distribution, density, -x) %>%
  ggplot(aes(x = x, y = density, color = distribution)) +
  geom_line()
```

Для нормального распределения со средним `0` и стандартным отклонением `1` --- вероятность того, что оно больше 2, составляет

```{r}
pnorm(2, lower.tail = FALSE)
```

В то время как в распределении Стьюдента, с числом степеней свободы = 2,

```{r}
pt(2, df = 2, lower.tail = FALSE)
```

#### Упражнение 23.2.1.2

<div class="question">
One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance:

```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  mean(abs(diff))
}
```

Use `optim()` to fit this model to the simulated data above and compare it to the linear model.
</div>

Чтобы вышеуказанная функция работала, нам нужно определить функцию `make_prediction()`, которая принимает числовой вектор длины два (пересечение и наклон) и возвращает предсказания,

```{r}
make_prediction <- function(mod, data) {
  mod[1] + mod[2] * data$x
}
```

Используя данные `sim1a`, лучшие параметры наименьшего абсолютного отклонения:

```{r}
best <- optim(c(0, 0), measure_distance, data = sim1a)
best$par
```

Используя данные `sim1a`, параметры целевой функции минимизации наименьших квадратов:

```{r}
measure_distance_ls <- function(mod, data) {
  diff <- data$y - (mod[1] + mod[2] * data$x)
  sqrt(mean(diff^2))
}

best <- optim(c(0, 0), measure_distance_ls, data = sim1a)
best$par
```

На практике предлагают не использовать `optim()` для соответствия этой модели, а вместо этого использовать существующую реализацию. 
Функции `rlm()` и `lqs()` в `MASS` подходят для надежных и устойчивых линейных моделей.

#### Упражнение 23.2.1.3

<div class="question">
One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optimum. What’s the problem with optimising a three parameter model like this?

```{r}
model3 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}
```
</div>

Проблема в том, что для любых значений `a[1] = a1` и `a[3] = a3`, любых других значений `a[1]` и `a[3]`, где `a[1] + a[3] == (a1 + а3)` будет иметь такую же посадку.

```{r}
measure_distance_3 <- function(a, data) {
  diff <- data$y - model3(a, data)
  sqrt(mean(diff^2))
}
```

В зависимости от наших отправных точек, мы можем найти различные оптимальные значения:

```{r}
best3a <- optim(c(0, 0, 0), measure_distance_3, data = sim1)
best3a$par

best3b <- optim(c(0, 0, 1), measure_distance_3, data = sim1)
best3b$par

best3c <- optim(c(0, 0, 5), measure_distance_3, data = sim1)
best3c$par
```

На самом деле существует бесконечное количество оптимальных значений для этой модели.

### Визуализация моделей

Если вашей целью является применение моделей для исследования статистических характеристик массивов данных, то вероятнее всего большую часть времени вы будете исследовать какие закономерности отражает модель, путём тщательного изучания семейства моделей.

Полезно бывает рассмотреть то, что моделью не охватывается, --- остатки., т.е. то что остается после вычитания прогнозных значений из фактических данных. Остатки предоставляют чрезвычайно полезную информацию, поскольку они позволяют использовать модели для устранения экстремальных пиков и тем самым дают возможность изучать слабые остаточные тренды.

#### Предсказания

Визуализацию предсказаний на основе модели мы начнём с того, что сгенерируем равномерную сетку значений охватывающих всю облать, в которой распологаются наши данные. Проще всего это можно сделать с помощью `modelr::data_grid()`.
Первый аргумент --- фрейм данных. Для последующих аргументов она находит уникальные переменные и генерирует все комбинации.

```{r}
grid <- sim1 %>%
  data_grid(x)
```

Станет ещё более понятно и интереснее, когда мы добавим переменные в нашу модель.

Следующий шаг --- добавление предсказаний. Сделаем это при помощи `modelr::add_predictions()`. Она принимает модель данных и сами данные. олученные с помощтю модели предсказания добавляет в новый столбец фрейма данных

```{r}
grid <- grid %>%
  add_predictions(sim1_mod) # sim1_mod --- это модель
```

Эту функцию можно использовать для добавления предсказаний в исходный набор данных. 

Далее мы откладываем предсказания в виде графика. 

```{r}
ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, color = "red", size = 1)
```

Такой подход будет работать с любой моделью в R. Здесь предсказание --- это не построение дополнительных значений, а построение зависимости, по которой эти дополнительные значения должны строится.

#### Остатки

Обратная сторона предсказаний, как завещал Тьюки --- остатки. Предсказания отражают закономерность, которую модели удалось уловить, тогда как остатки предоставляют информацию о том, что не охватывается моделью.

Остатки --- это не что иное, как расстояния между наблюдениями и вычисленными нами прогнозными значениями.

Мы добавляем остатки к данным с помощью функции `modelr::add_residuals()`, которая работает во многом аналогично функции `modelr::add_predictions()`. Стоит отметить --- что мы используем исходный набор данных, а не искуственную сетку.

Это обсуловлено тем, что для вычисления остатков нам нужны фактические значения `y`

```{r}
sim1 <- sim1 %>%
  add_residuals(sim1_mod)
```

Существует несколько способов извлечения информации о модели на основе остатков. Один из них --- графические представление распределения остатков с помощю полигона частот. 

```{r}
ggplot(sim1, aes(resid)) + 
  geom_freqpoly(binwidth = 0.5)
```

Это ползволяет калибровать качество  модели: насколько прогноз отличается от наблюдаемых значений? Обратите внимание на то, что среднее остатков всегда равно 0.

Потребность в построении графиков с использование остатков вместо исходного предиктора будет возникать довольно часто.

```{r}
ggplot(sim1, aes(x, resid)) + 
  geom_ref_line(h = 0) +
  geom_point()
```

Этот график выглядит как случайный шум. Это косвенно говорит о том, что наша модель неплохо справилась со своей задачей определения закономерности в поведении данных.

#### Упражнение 23.3.3.1
<div class="question">
Instead of using `lm()` to fit a straight line, you can use `loess()` to fit a smooth curve. 
Repeat the process of model fitting, grid generation, predictions, and visualisation on `sim1` using `loess()` instead of `lm()`.
How does the result compare to `geom_smooth()`?
</div>

Сначала произведём подгонку модели

```{r}
sim1_loess <- loess(y ~ x, data = sim1)
sim1_lm <- lm(y ~ x, data = sim1)
```

Теперь сгенерируем сетку

```{r}
grid_loess <- sim1 %>%
  add_predictions(sim1_loess)
```

Теперь спрогнозируем

```{r}
sim1 <- sim1 %>%
  add_residuals(sim1_lm) %>%
  add_predictions(sim1_lm) %>%
  add_residuals(sim1_loess, var = "resid_loess") %>%
  add_predictions(sim1_loess, var = "pred_loess")
```

И после этого визуализируем. На одном графике, для сравнения, я построил три модели. Зелёная линейная, синяя --- гладкая при помощи функции `geom_smooth()`. И красная, при помощи функции `loess()`.

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, color = "grey30") + 
  geom_smooth(se = F, size = 2, color = "blue") +
  geom_line(aes(x, pred), data = grid_loess, color = "red") +
  geom_line(aes(y = pred), data = grid, color = "green")
```

И наконец, построим остатки для гладкой модели и сравним их с остатками из прямолинейной модели (черный цвет).

```{r}
ggplot(sim1, aes(x = x)) +
  geom_ref_line(h = 0) +
  geom_point(aes(y = resid)) +
  geom_point(aes(y = resid_loess), colour = "red")
```

В целом, довольно схоже.

#### Упражнение 23.3.3.2
<div class="question">
`add_predictions()` is paired with `gather_predictions()` and `spread_predictions()`. 
How do these three functions differ?
</div>

Функции `collect_predictions()` и `spread_predictions()` позволяют добавлять прогнозы из нескольких моделей одновременно.
Например

```{r}
sim1_mod <- lm(y ~ x, data = sim1)
grid <- sim1 %>%
  data_grid(x)
```

Функция `add_predictions()` добавляет только одну модель за раз. Чтобы добавить две модели:

```{r}
grid %>%
  add_predictions(sim1_mod, var = "pred_lm") %>%
  add_predictions(sim1_loess, var = "pred_loess")
```

Функция `collect_predictions()` добавляет прогнозы из нескольких моделей, складывая результаты и добавляя столбец с именем модели

```{r}
grid %>%
  gather_predictions(sim1_mod, sim1_loess)
```

Функция `spread_predictions()` добавляет прогнозы из нескольких моделей, добавляя несколько столбцов (с добавлением названия модели) с прогнозами из каждой модели.

```{r}
grid %>%
  spread_predictions(sim1_mod, sim1_loess)
```

Функция `spread_predictions()` аналогична примеру, который запускает `add_predictions()` для каждой модели, и эквивалентна запуску `spread()` после запуска `collect_predictions()`:

```{r}
grid %>%
  gather_predictions(sim1_mod, sim1_loess) %>%
  spread(model, pred)
```

#### Упражнение 23.3.3.3
<div class="question">
What does `geom_ref_line()` do? 
What package does it come from? 
Why is displaying a reference line in plots showing residuals useful and important?
</div>

`geom_ref_line()` добавляет в качестве опорной линии на участок.
В примере выше это был ноль.
В принципе, это эквивалентно запуску `geom_hline()` или `geom_vline()` с настройками по умолчанию, которые полезны для визуализации моделей. 
Поместить опорную линию в ноль для остатков важно, потому что хорошие модели (как правило) должны иметь остатки, центрированные в нуле, с примерно той же дисперсией (или распределением) по отношению к поддержке `x` и без корреляции. 
Нулевая контрольная линия облегчает визуальную оценку этих характеристик.

#### Упражнение 23.3.3.4
<div class="question">
Why might you want to look at a frequency polygon of absolute residuals? W
hat are the pros and cons compared to looking at the raw residuals?
</div>

Отображение абсолютных значений остатков облегчает просмотр распределения остатков. 
Модель предполагает, что остатки имеют средний ноль, а использование абсолютных значений остатков фактически удваивает число остатков.

```{r}
sim1_mod <- lm(y ~ x, data = sim1)

sim1 <- sim1 %>%
  add_residuals(sim1_mod)

ggplot(sim1, aes(x = abs(resid))) +
  geom_freqpoly(binwidth = 0.5)
```

Однако использование абсолютных значений невязок отбрасывает информацию о знаке, а это означает, что многоугольник частоты не может показать, насколько модель систематически переоценивает или занижает невязки.

### Формулы и семейства моделей

ДО этого мы уже сталкивались с формулами, когда использовали функции `facet_wrap()` и `facet_grid()`. В R формулы обуспечивают обзий способ получения специального поведения.

Вместо того чтобы сразу же использовать переменные в вычислениях, они захватыввают их для последующей интепретации с помощью функции.

Большинство функций моделирования в R используют стандартное преобразование от формул к функциям. Один из примеров простого преобразования:

`y ~ x` --- `y = a_1 + a_2 * x`

Если нужно увидеть что фактически делает R, можно использовать функцию `model_matrix()`.
Эта функция принимает фрейм данных и формулу и возвращает tibble-фрейм, определяющий уравнение модели: каждый столбец результата связан с одним коэффициентом модели, а функция всегда имеет вид `y = a_1 * out + a_2 * out_2`.
Для простого случая `y ~ x1` мы получаем нечто любопытное.

```{r}
df <- tribble(
  ~y, ~x1, ~x2,
  4, 2, 5,
  5, 1, 6
)
model_matrix(df, y ~ x1)
```

По умолчанию R добавляет столбец заполненный единицами. Если гнужно отказаться от него добавим в формулу `-1`

```{r}
model_matrix(df, y ~ x1 - 1)
```

При добавлении дополнительных переменных в модель размер матрицы модели, как и следовало ожидать, увеличивается

```{r}
model_matrix(df, y ~ x1 + x2)
```

Здесь <https://www.jstor.org/stable/2346786?read-now=1&seq=1#metadata_info_tab_contents> подробнее о символьных преобразованиях.

#### Категориальные переменные

Если предиктором является непрерывная переменная, то генерация функции из формулы осуществляется непосредственным образом, но если предикатор --- категориальная переменная, то всё немного усложняется.

Предположим, имеется формула наподобие `y ~ sex`, где переменная `sex` --- может принимать два значения, которые обозначают мужчину и женщину (`male` и `female`). Преобразовать эту формулу к виду `y = x_1 + x_2 * sex` не имеет смысла, поскольку переменная `sex` не является числом --- она просто не может участвовать в умножении. Вместо этого R преобразует её к виду `y = x_1 + x_2 * sex_male`, где sex_male принимает 1 для мужчин и 0 для женщин.

```{r}
df <- tribble(
  ~ sex, ~ response,
  "male", 1,
  "female", 2,
  "male", 1
)
model_matrix(df, response ~ sex)
```

Используем набор данных `sim2` для того чтобы конкретизировать общие утверждения о точной параметризации.

```{r}
ggplot(sim2) + 
  geom_point(aes(x, y))
```

Мы можем подобрать модель для этого набора данных и сгенерировать прогноз.

```{r}
mod2 <- lm(y ~ x, data = sim2)

grid <- sim2 %>% 
  data_grid(x) %>% 
  add_predictions(mod2)
grid
```

В конечном счете модель с категориальной переменной `x` будет предсказывать среднее значение для каждой категории. (Почему? потому что среднее минимизирует среднеквадратическое отклонение). Это можно легко увидеть, если наложить предсказания на исходные данные.

```{r}
ggplot(sim2, aes(x)) + 
  geom_point(aes(y = y)) +
  geom_point(data = grid, aes(y = pred), colour = "red", size = 4)
```

Вы не можете делать прогнозы относительно уровней, которые не наблюдали. Иногда это будет происходить случайно, поэтому стоит запомнить следующее сообщение об ошибке.

```{r}
tibble(x = "e") %>% 
  add_predictions(mod2)
```

#### Взаимодействия --- непрерывная и категориальная переменные

Что происходит в тех случаях, когда непрерывная переменная сочетается с категориальной?

```{r}
ggplot(sim3, aes(x1, y)) +
  geom_point(aes(color = x2))
```

Для этих данных возможны две модели

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)
```

КОгда добавляешь модели с помощью знака `+` они оценивают каждый эффект независимо от всех остальных. Для подгонки так называемого взаиомодействия можно использовать знак `*`. Например `y ~ x1 * x2` транслируется в `y = a_0 + a_1 * x1 + a_2 * x2 + a_12 * x1 * x2`.
Когда используешь знак `*` в модель включается как взаимодействие, так и индивидуальная компонента.

Для визуализации понадобится два новшества

* У нас два предиктора, поэтому мы должны предоставить функции `data_grid()` обе переменные. Эта функция найдёт все уникальные значения `x1` и `x2` и сгенерирует все их комбинации

* Чтобы сгенерировать прогнозы на основе сразу двух моделей, можно использовать функцию `gather_predictions()`, которая добавляет каждый прогноз в качестве строки. Комплементарной к функции `gather_predictions()` является `spread_predictions()`, которая добавляет каждый прогноз в новый столбец.

Вместе это даёт нам следующее

```{r}
grid <- sim3 %>% 
  data_grid(x1, x2) %>% 
  gather_predictions(mod1, mod2)
```

Результаты для обеих моделей можно визуализировать на одном графике

```{r}
ggplot(sim3, aes(x1, y, colour = x2)) + 
  geom_point() + 
  geom_line(data = grid, aes(y = pred)) + 
  facet_wrap(~ model)
```

ВТорая модель интуитивно кажется более корректной.
Стоит обратить внимание, что во второй модели все прямые имеют разный угол наклона, тогда как при использовании `+` в `mod1` все прямые наклонены под одним углом но с разным intercept.

Чтобы убедиться в том, какая модель лучше описывает данные можно рассмотреть остатки. 

```{r}
sim3 <- sim3 %>% 
  gather_residuals(mod1, mod2)

ggplot(sim3, aes(x1, resid, colour = x2)) + 
  geom_point() + 
  facet_grid(model ~ x2)
```

В остальном, нужна большая математическая подготовка, чтобы ответить на то, насколько вторая модель лучше чем первая.

