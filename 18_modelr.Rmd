---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Моделирование

Знакомимся с Exploratory data analysis (EDA) --- разведочный анализ данных.

Цель моделирования --- получение простых суммарных (сводных) характеристик набора данных.

Здесь разбриаем предсказательные модели анализа данных. Есть ещё много разных, например data discovery, но их здесь не рассматриваем

Базовые принципы корректного статистического вывода:

* Каждое наблюдение можно использовать либо для разведочного анализа, либо для подтверждения гипотезы. Но не для того и другого одновременно

* В целях разведочного анализа допускается многократное использование наблюдения, но в целях подтверждения гипотез --- только однократное. Использование одного и того же наблюдения более одного раза равносильно преходу от подтверждения гипотезы к разведочному анализу.

Данные используемые для подтверждения гипотезы не должны зависеть от данных на основании которых эта гипотеза была выдвинута.

### Выдвижение гипотез и их подтверждение

Один из подходов к анализу, направленному на подтверждение гипотезы, заключается в разбиении данных на три части ещё до того, как присутпить к анализу

* $60%$ --- training set. С этим набором можноделать всё что пожелается.

* $20%$ --- query set. Набор для сравнения моделей или вариантов визуализации вручную, но их не разрешается использовать в качестве части автоматизированного процесса.

* $20%$ --- test set. Эти данные можно использовать только один раз для тестирования окончательной модели.

## Базовое моделирование при помощи пакета `modelr`
### Введение

Цель моделирования --- определение суммарных характеристик набора данных. 

Моделирование включает два аспекта

1. Прежде всего необходимо определить семейство моделей. Они определяют типичную закономерность, например линейную или квадратичную зависимость. Семейство моделей описывается уравнением наподобие `y = a_1 * x + a_2` or `y = a_1 * x ^ a_2`. Где `x` и `y` --- известные переменные, а `a_1` и `a_2` варьируемые параметры.

2. Далее вы генерируете подходяющую модель, выбирая ту из семейства моделей, которая больше всего соответствует данным. В результате этого обобщенного уравнения семейства моделей конкретизируется принимая вид наподобие `y = 3 * x + 7`

Целью моделирования является не установление окончательной истины, а нахождение простого, но тем не менее полезного приближения


#### Необходимые ресурсы

```{r}
library(tidyverse)

library(modelr)
options(na.action = na.warn)
```

### Простая модель

Демонстрационный набор данный `sim1` содержит две непрерывные переменные `x` и `y`. Отложим их на графике, чтобы увидеть, как они связаны между собой

```{r}
ggplot(sim1, aes(x, y)) +
  geom_point()
```

между переменными наблюдается сильная взаимосвязь. Коэффициент корреляции

```{r}
cor(sim1$x, sim1$y)
```

Опишем эту взаимосвязь с помощью модели и выразим ее в явном виде.

Наша задача --- предоставить базовую форму модели. В данном случае мы используем линейное соотношение между переменными `y = a_0 + a_1 * x`.
Начнем с получения общего представления о том, как выглядят модели этого семейства. Путём случайной генерации некоторых из них и наложения их на данные.

В нашем случае можно использовать `geom_abline` --- она принимает наклон (slope), и y-пересечение (intercept).

```{r}
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

ggplot(sim1, aes(x, y)) + 
  geom_abline(
    aes(intercept = a1, slope = a2), 
    data = models, alpha = 1/4
  ) +
  geom_point()
```

Большинство из построенных моделей совсем неудачны. Нам нужен метод обеспечивающий количественную оценку расстояния между данными и моделью.

Это расстояние --- разность между значением предоставляемым моделью --- прогноз, и фактическим значением `y` согласно данным `data` --- результат.

![расстояния от результата до прогноза](img/model.png)

Для вычисления расстояния мы прежде всего превратим семейство моделей в функцию. 
В качестве аргументов, эта функция принимает параметры модели и данные, а в качестве выходного результата предоставляет значения, предсказанные моделью

```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

model1(c(7, 1.5), sim1)
```

Чтобы найти минимальное расстояние при помощи среднеквадратического отклонения.

```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}
measure_distance(c(7, 1.5), sim1)
```

Теперь используем пакет `purrr` для того чтобы вычислить это расстояние для всех ранее определённых моделей. Нам нужна вспомогательная функция, поскольку наша функция вычисляющая расстояние лжидает модель в качестве числового вектора с длиной 2

```{r}
sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}

models <- models %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
models
```

Выделим лучшие модели

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, color = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, color = -dist), 
    data = filter(models, rank(dist) <= 10)
  )
```

Мы можем рассматривать эти модели как наблюдения и визуализировать их с помощью точечной диаграмы с осями `a1` и `a2`, опять таки расцветив их в соответстсвии со значениями `-dist`. При этом мы увидим множество моделей одновременно.
Выделим 10 лучшиъ моделей кржочками

```{r}
ggplot(models, aes(a1, a2)) +
  geom_point(data = filter(models, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))
```

Можно так же построить сетку поиска! Где точки расположены не хаотично, а упорядоченно

```{r}
grid <- expand.grid(
  a1 = seq(-2, 10, length = 25),
  a2 = seq(0, 2, length = 25)
  ) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>% 
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist)) 
```

Если наложить эти 10 наилучших моделей на исходные данные, то они будут выглядеть довольно прилично

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(grid, rank(dist) <= 10)
  )
```

но конечно самый простой способ это воспользоваться  функцией `lm()` для построения линейных моделей

```{r}
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
```

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = coef(sim1_mod)[1], slope = coef(sim1_mod)[2])
```

#### Упражнение 23.2.1.1

<div class="question">
One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?

```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
```
</div>

Линейные модели неустойчивы к выбросам --- известный факт.

Попробуем это на одиночной модели

```{r}
sim1a_mod <- lm(y ~ x, sim1a)

ggplot(sim1a, aes(x, y)) + 
  geom_point(size = 2, color = "grey30") + 
  geom_abline(intercept = coef(sim1a_mod)[1], slope = coef(sim1a_mod)[2], color = "red")
```

Сделаем функцию, которая бы генерировала несколько распределений сразу

```{r}
simt <- function(i) {
  tibble(
    x = rep(1:10, each = 3),
    y = x * 1.5 + 6 + rt(length(x), df = 2),
    id = i
  )
}

sims <- map_df(1:15, simt)

ggplot(sims, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  facet_wrap(~id, ncol = 5)
```

Что если проделать всё тоже самое с нормальным распределением

```{r}
sim_norm <- function(i) {
  tibble(
    x = rep(1:10, each = 3),
    y = x * 1.5 + 6 + rnorm(length(x)),
    .id = i
  )
}

simdf_norm <- map_df(1:12, sim_norm)

ggplot(simdf_norm, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "red") +
  facet_wrap(~.id, ncol = 4)
```

Здесь не большие выбросы, а склоны больше похожи.

Причина в том, что t-распределение Стьюдента, из которого мы выбираем с помощью `rt()`, имеет более тяжелые хвосты, чем нормальное распределение `rnorm()`. Это означает, что t-распределение Стьюдента присваивает большую вероятность значениям дальше от центра распределения.

```{r}
tibble(
  x = seq(-5, 5, length.out = 100),
  normal = dnorm(x),
  student_t = dt(x, df = 2)
) %>%
  gather(distribution, density, -x) %>%
  ggplot(aes(x = x, y = density, color = distribution)) +
  geom_line()
```

Для нормального распределения со средним `0` и стандартным отклонением `1` --- вероятность того, что оно больше 2, составляет

```{r}
pnorm(2, lower.tail = FALSE)
```

В то время как в распределении Стьюдента, с числом степеней свободы = 2,

```{r}
pt(2, df = 2, lower.tail = FALSE)
```

#### Упражнение 23.2.1.2

<div class="question">
One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance:

```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  mean(abs(diff))
}
```

Use `optim()` to fit this model to the simulated data above and compare it to the linear model.
</div>

Чтобы вышеуказанная функция работала, нам нужно определить функцию `make_prediction()`, которая принимает числовой вектор длины два (пересечение и наклон) и возвращает предсказания,

```{r}
make_prediction <- function(mod, data) {
  mod[1] + mod[2] * data$x
}
```

Используя данные `sim1a`, лучшие параметры наименьшего абсолютного отклонения:

```{r}
best <- optim(c(0, 0), measure_distance, data = sim1a)
best$par
```

Используя данные `sim1a`, параметры целевой функции минимизации наименьших квадратов:

```{r}
measure_distance_ls <- function(mod, data) {
  diff <- data$y - (mod[1] + mod[2] * data$x)
  sqrt(mean(diff^2))
}

best <- optim(c(0, 0), measure_distance_ls, data = sim1a)
best$par
```

На практике предлагают не использовать `optim()` для соответствия этой модели, а вместо этого использовать существующую реализацию. 
Функции `rlm()` и `lqs()` в `MASS` подходят для надежных и устойчивых линейных моделей.

#### Упражнение 23.2.1.3

<div class="question">
One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optimum. What’s the problem with optimising a three parameter model like this?

```{r}
model3 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}
```
</div>

Проблема в том, что для любых значений `a[1] = a1` и `a[3] = a3`, любых других значений `a[1]` и `a[3]`, где `a[1] + a[3] == (a1 + а3)` будет иметь такую же посадку.

```{r}
measure_distance_3 <- function(a, data) {
  diff <- data$y - model3(a, data)
  sqrt(mean(diff^2))
}
```

В зависимости от наших отправных точек, мы можем найти различные оптимальные значения:

```{r}
best3a <- optim(c(0, 0, 0), measure_distance_3, data = sim1)
best3a$par

best3b <- optim(c(0, 0, 1), measure_distance_3, data = sim1)
best3b$par

best3c <- optim(c(0, 0, 5), measure_distance_3, data = sim1)
best3c$par
```

На самом деле существует бесконечное количество оптимальных значений для этой модели.

### Визуализация моделей

Если вашей целью является применение моделей для исследования статистических характеристик массивов данных, то вероятнее всего большую часть времени вы будете исследовать какие закономерности отражает модель, путём тщательного изучания семейства моделей.

Полезно бывает рассмотреть то, что моделью не охватывается, --- остатки., т.е. то что остается после вычитания прогнозных значений из фактических данных. Остатки предоставляют чрезвычайно полезную информацию, поскольку они позволяют использовать модели для устранения экстремальных пиков и тем самым дают возможность изучать слабые остаточные тренды.

#### Предсказания

Визуализацию предсказаний на основе модели мы начнём с того, что сгенерируем равномерную сетку значений охватывающих всю облать, в которой распологаются наши данные. Проще всего это можно сделать с помощью `modelr::data_grid()`.
Первый аргумент --- фрейм данных. Для последующих аргументов она находит уникальные переменные и генерирует все комбинации.

```{r}
grid <- sim1 %>%
  data_grid(x)
```

Станет ещё более понятно и интереснее, когда мы добавим переменные в нашу модель.

Следующий шаг --- добавление предсказаний. Сделаем это при помощи `modelr::add_predictions()`. Она принимает модель данных и сами данные. олученные с помощтю модели предсказания добавляет в новый столбец фрейма данных

```{r}
grid <- grid %>%
  add_predictions(sim1_mod) # sim1_mod --- это модель
```

Эту функцию можно использовать для добавления предсказаний в исходный набор данных. 

Далее мы откладываем предсказания в виде графика. 

```{r}
ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, color = "red", size = 1)
```

Такой подход будет работать с любой моделью в R. Здесь предсказание --- это не построение дополнительных значений, а построение зависимости, по которой эти дополнительные значения должны строится.

#### Остатки

Обратная сторона предсказаний, как завещал Тьюки --- остатки. Предсказания отражают закономерность, которую модели удалось уловить, тогда как остатки предоставляют информацию о том, что не охватывается моделью.

Остатки --- это не что иное, как расстояния между наблюдениями и вычисленными нами прогнозными значениями.

Мы добавляем остатки к данным с помощью функции `modelr::add_residuals()`, которая работает во многом аналогично функции `modelr::add_predictions()`. Стоит отметить --- что мы используем исходный набор данных, а не искуственную сетку.

Это обсуловлено тем, что для вычисления остатков нам нужны фактические значения `y`

```{r}
sim1 <- sim1 %>%
  add_residuals(sim1_mod)
```

Существует несколько способов извлечения информации о модели на основе остатков. Один из них --- графические представление распределения остатков с помощю полигона частот. 

```{r}
ggplot(sim1, aes(resid)) + 
  geom_freqpoly(binwidth = 0.5)
```

Это ползволяет калибровать качество  модели: насколько прогноз отличается от наблюдаемых значений? Обратите внимание на то, что среднее остатков всегда равно 0.

Потребность в построении графиков с использование остатков вместо исходного предиктора будет возникать довольно часто.

```{r}
ggplot(sim1, aes(x, resid)) + 
  geom_ref_line(h = 0) +
  geom_point()
```

Этот график выглядит как случайный шум. Это косвенно говорит о том, что наша модель неплохо справилась со своей задачей определения закономерности в поведении данных.

#### Упражнение 23.3.3.1
<div class="question">
Instead of using `lm()` to fit a straight line, you can use `loess()` to fit a smooth curve. 
Repeat the process of model fitting, grid generation, predictions, and visualisation on `sim1` using `loess()` instead of `lm()`.
How does the result compare to `geom_smooth()`?
</div>

Сначала произведём подгонку модели

```{r}
sim1_loess <- loess(y ~ x, data = sim1)
sim1_lm <- lm(y ~ x, data = sim1)
```

Теперь сгенерируем сетку

```{r}
grid_loess <- sim1 %>%
  add_predictions(sim1_loess)
```

Теперь спрогнозируем

```{r}
sim1 <- sim1 %>%
  add_residuals(sim1_lm) %>%
  add_predictions(sim1_lm) %>%
  add_residuals(sim1_loess, var = "resid_loess") %>%
  add_predictions(sim1_loess, var = "pred_loess")
```

И после этого визуализируем. На одном графике, для сравнения, я построил три модели. Зелёная линейная, синяя --- гладкая при помощи функции `geom_smooth()`. И красная, при помощи функции `loess()`.

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, color = "grey30") + 
  geom_smooth(se = F, size = 2, color = "blue") +
  geom_line(aes(x, pred), data = grid_loess, color = "red") +
  geom_line(aes(y = pred), data = grid, color = "green")
```

И наконец, построим остатки для гладкой модели и сравним их с остатками из прямолинейной модели (черный цвет).

```{r}
ggplot(sim1, aes(x = x)) +
  geom_ref_line(h = 0) +
  geom_point(aes(y = resid)) +
  geom_point(aes(y = resid_loess), colour = "red")
```

В целом, довольно схоже.

#### Упражнение 23.3.3.2
<div class="question">
`add_predictions()` is paired with `gather_predictions()` and `spread_predictions()`. 
How do these three functions differ?
</div>

Функции `collect_predictions()` и `spread_predictions()` позволяют добавлять прогнозы из нескольких моделей одновременно.
Например

```{r}
sim1_mod <- lm(y ~ x, data = sim1)
grid <- sim1 %>%
  data_grid(x)
```

Функция `add_predictions()` добавляет только одну модель за раз. Чтобы добавить две модели:

```{r}
grid %>%
  add_predictions(sim1_mod, var = "pred_lm") %>%
  add_predictions(sim1_loess, var = "pred_loess")
```

Функция `collect_predictions()` добавляет прогнозы из нескольких моделей, складывая результаты и добавляя столбец с именем модели

```{r}
grid %>%
  gather_predictions(sim1_mod, sim1_loess)
```

Функция `spread_predictions()` добавляет прогнозы из нескольких моделей, добавляя несколько столбцов (с добавлением названия модели) с прогнозами из каждой модели.

```{r}
grid %>%
  spread_predictions(sim1_mod, sim1_loess)
```

