---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Моделирование

Знакомимся с Exploratory data analysis (EDA) --- разведочный анализ данных.

Цель моделирования --- получение простых суммарных (сводных) характеристик набора данных.

Здесь разбриаем предсказательные модели анализа данных. Есть ещё много разных, например data discovery, но их здесь не рассматриваем

Базовые принципы корректного статистического вывода:

* Каждое наблюдение можно использовать либо для разведочного анализа, либо для подтверждения гипотезы. Но не для того и другого одновременно

* В целях разведочного анализа допускается многократное использование наблюдения, но в целях подтверждения гипотез --- только однократное. Использование одного и того же наблюдения более одного раза равносильно преходу от подтверждения гипотезы к разведочному анализу.

Данные используемые для подтверждения гипотезы не должны зависеть от данных на основании которых эта гипотеза была выдвинута.

### Выдвижение гипотез и их подтверждение

Один из подходов к анализу, направленному на подтверждение гипотезы, заключается в разбиении данных на три части ещё до того, как присутпить к анализу

* $60%$ --- training set. С этим набором можноделать всё что пожелается.

* $20%$ --- query set. Набор для сравнения моделей или вариантов визуализации вручную, но их не разрешается использовать в качестве части автоматизированного процесса.

* $20%$ --- test set. Эти данные можно использовать только один раз для тестирования окончательной модели.

## Базовое моделирование при помощи пакета `modelr`
### Введение

Цель моделирования --- определение суммарных характеристик набора данных. 

Моделирование включает два аспекта

1. Прежде всего необходимо определить семейство моделей. Они определяют типичную закономерность, например линейную или квадратичную зависимость. Семейство моделей описывается уравнением наподобие `y = a_1 * x + a_2` or `y = a_1 * x ^ a_2`. Где `x` и `y` --- известные переменные, а `a_1` и `a_2` варьируемые параметры.

2. Далее вы генерируете подходяющую модель, выбирая ту из семейства моделей, которая больше всего соответствует данным. В результате этого обобщенного уравнения семейства моделей конкретизируется принимая вид наподобие `y = 3 * x + 7`

Целью моделирования является не установление окончательной истины, а нахождение простого, но тем не менее полезного приближения


#### Необходимые ресурсы

```{r}
library(tidyverse)

library(modelr)
options(na.action = na.warn)
```

### Простая модель

Демонстрационный набор данный `sim1` содержит две непрерывные переменные `x` и `y`. Отложим их на графике, чтобы увидеть, как они связаны между собой

```{r}
ggplot(sim1, aes(x, y)) +
  geom_point()
```

между переменными наблюдается сильная взаимосвязь. Коэффициент корреляции

```{r}
cor(sim1$x, sim1$y)
```

Опишем эту взаимосвязь с помощью модели и выразим ее в явном виде.

Наша задача --- предоставить базовую форму модели. В данном случае мы используем линейное соотношение между переменными `y = a_0 + a_1 * x`.
Начнем с получения общего представления о том, как выглядят модели этого семейства. Путём случайной генерации некоторых из них и наложения их на данные.

В нашем случае можно использовать `geom_abline` --- она принимает наклон (slope), и y-пересечение (intercept).

```{r}
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

ggplot(sim1, aes(x, y)) + 
  geom_abline(
    aes(intercept = a1, slope = a2), 
    data = models, alpha = 1/4
  ) +
  geom_point()
```

Большинство из построенных моделей совсем неудачны. Нам нужен метод обеспечивающий количественную оценку расстояния между данными и моделью.

Это расстояние --- разность между значением предоставляемым моделью --- прогноз, и фактическим значением `y` согласно данным `data` --- результат.

![расстояния от результата до прогноза](img/model.png)

Для вычисления расстояния мы прежде всего превратим семейство моделей в функцию. 
В качестве аргументов, эта функция принимает параметры модели и данные, а в качестве выходного результата предоставляет значения, предсказанные моделью

```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

model1(c(7, 1.5), sim1)
```

Чтобы найти минимальное расстояние при помощи среднеквадратического отклонения.

```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}
measure_distance(c(7, 1.5), sim1)
```

Теперь используем пакет `purrr` для того чтобы вычислить это расстояние для всех ранее определённых моделей. Нам нужна вспомогательная функция, поскольку наша функция вычисляющая расстояние лжидает модель в качестве числового вектора с длиной 2

```{r}
sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}

models <- models %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))
models
```

Выделим лучшие модели

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, color = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, color = -dist), 
    data = filter(models, rank(dist) <= 10)
  )
```

Мы можем рассматривать эти модели как наблюдения и визуализировать их с помощью точечной диаграмы с осями `a1` и `a2`, опять таки расцветив их в соответстсвии со значениями `-dist`. При этом мы увидим множество моделей одновременно.
Выделим 10 лучшиъ моделей кржочками

```{r}
ggplot(models, aes(a1, a2)) +
  geom_point(data = filter(models, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))
```

Можно так же построить сетку поиска! Где точки расположены не хаотично, а упорядоченно

```{r}
grid <- expand.grid(
  a1 = seq(-2, 10, length = 25),
  a2 = seq(0, 2, length = 25)
  ) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>% 
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist)) 
```

Если наложить эти 10 наилучших моделей на исходные данные, то они будут выглядеть довольно прилично

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(grid, rank(dist) <= 10)
  )
```

но конечно самый простой способ это воспользоваться  функцией `lm()` для построения линейных моделей

```{r}
sim_mod <- lm(y ~ x, sim1)
coef(sim_mod)
```

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = coef(sim_mod)[1], slope = coef(sim_mod)[2])
```

#### Упражнение 23.2.1.1

<div class="question">
One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?

```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
```
</div>

Линейные модели неустойчивы к выбросам --- известный факт.

Попробуем это на одиночной модели

```{r}
sim1a_mod <- lm(y ~ x, sim1a)

ggplot(sim1a, aes(x, y)) + 
  geom_point(size = 2, color = "grey30") + 
  geom_abline(intercept = coef(sim1a_mod)[1], slope = coef(sim1a_mod)[2], color = "red")
```

Сделаем функцию, которая бы генерировала несколько распределений сразу

```{r}
simt <- function(i) {
  tibble(
    x = rep(1:10, each = 3),
    y = x * 1.5 + 6 + rt(length(x), df = 2),
    id = i
  )
}

sims <- map_df(1:15, simt)

ggplot(sims, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  facet_wrap(~id, ncol = 5)
```

Что если проделать всё тоже самое с нормальным распределением

```{r}
sim_norm <- function(i) {
  tibble(
    x = rep(1:10, each = 3),
    y = x * 1.5 + 6 + rnorm(length(x)),
    .id = i
  )
}

simdf_norm <- map_df(1:12, sim_norm)

ggplot(simdf_norm, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "red") +
  facet_wrap(~.id, ncol = 4)
```

Здесь не большие выбросы, а склоны больше похожи.

Причина в том, что t-распределение Стьюдента, из которого мы выбираем с помощью `rt()`, имеет более тяжелые хвосты, чем нормальное распределение `rnorm()`. Это означает, что t-распределение Стьюдента присваивает большую вероятность значениям дальше от центра распределения.

```{r}
tibble(
  x = seq(-5, 5, length.out = 100),
  normal = dnorm(x),
  student_t = dt(x, df = 2)
) %>%
  gather(distribution, density, -x) %>%
  ggplot(aes(x = x, y = density, color = distribution)) +
  geom_line()
```

Для нормального распределения со средним `0` и стандартным отклонением `1` --- вероятность того, что оно больше 2, составляет

```{r}
pnorm(2, lower.tail = FALSE)
```

В то время как в распределении Стьюдента, с числом степеней свободы = 2,

```{r}
pt(2, df = 2, lower.tail = FALSE)
```

#### Упражнение 23.2.1.2

<div class="question">
One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance:

```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  mean(abs(diff))
}
```

Use `optim()` to fit this model to the simulated data above and compare it to the linear model.
</div>

Чтобы вышеуказанная функция работала, нам нужно определить функцию `make_prediction()`, которая принимает числовой вектор длины два (пересечение и наклон) и возвращает предсказания,

```{r}
make_prediction <- function(mod, data) {
  mod[1] + mod[2] * data$x
}
```

Используя данные `sim1a`, лучшие параметры наименьшего абсолютного отклонения:

```{r}
best <- optim(c(0, 0), measure_distance, data = sim1a)
best$par
```

Используя данные `sim1a`, параметры целевой функции минимизации наименьших квадратов:

```{r}
measure_distance_ls <- function(mod, data) {
  diff <- data$y - (mod[1] + mod[2] * data$x)
  sqrt(mean(diff^2))
}

best <- optim(c(0, 0), measure_distance_ls, data = sim1a)
best$par
```

На практике предлагают не использовать `optim()` для соответствия этой модели, а вместо этого использовать существующую реализацию. 
Функции `rlm()` и `lqs()` в `MASS` подходят для надежных и устойчивых линейных моделей.

#### Упражнение 23.2.1.3

<div class="question">
One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optimum. What’s the problem with optimising a three parameter model like this?

```{r}
model3 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}
```
</div>

Проблема в том, что для любых значений `a[1] = a1` и `a[3] = a3`, любых других значений `a[1]` и `a[3]`, где `a[1] + a[3] == (a1 + а3)` будет иметь такую же посадку.

```{r}
measure_distance_3 <- function(a, data) {
  diff <- data$y - model3(a, data)
  sqrt(mean(diff^2))
}
```

В зависимости от наших отправных точек, мы можем найти различные оптимальные значения:

```{r}
best3a <- optim(c(0, 0, 0), measure_distance_3, data = sim1)
best3a$par

best3b <- optim(c(0, 0, 1), measure_distance_3, data = sim1)
best3b$par

best3c <- optim(c(0, 0, 5), measure_distance_3, data = sim1)
best3c$par
```

На самом деле существует бесконечное количество оптимальных значений для этой модели.

### Визуализация моделей

Если вашей целью является применение моделей для исследования статистических характеристик массивов данных, то вероятнее всего большую часть времени вы будете исследовать какие закономерности отражает модель, путём тщательного изучания семейства моделей